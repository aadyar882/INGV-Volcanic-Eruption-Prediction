{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as mse\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.preprocessing import StandardScaler\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/ingv-data/Train.csv')\ntimes = train.iloc[:,1]\ntimes\n\n#final shape of training data: 4431 x 75","metadata":{"execution":{"iopub.status.busy":"2022-06-22T20:29:47.754144Z","iopub.execute_input":"2022-06-22T20:29:47.754648Z","iopub.status.idle":"2022-06-22T20:29:50.581382Z","shell.execute_reply.started":"2022-06-22T20:29:47.754611Z","shell.execute_reply":"2022-06-22T20:29:50.580177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ![](http://www.ready.gov/sites/default/files/2019-09/volcano.jpg)**Step 1: Feature Engineering**","metadata":{}},{"cell_type":"code","source":"missing_values_count = train.isnull().sum()\nmissing_values_count","metadata":{"execution":{"iopub.status.busy":"2022-06-22T20:24:26.315088Z","iopub.execute_input":"2022-06-22T20:24:26.316088Z","iopub.status.idle":"2022-06-22T20:24:26.324817Z","shell.execute_reply.started":"2022-06-22T20:24:26.316043Z","shell.execute_reply":"2022-06-22T20:24:26.324012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_frags = glob.glob(\"../input/predict-volcanic-eruptions-ingv-oe/train/*\")\ntest_frags = glob.glob(\"../input/predict-volcanic-eruptions-ingv-oe/test/*\")\ncheck = pd.read_csv('../input/predict-volcanic-eruptions-ingv-oe/train/2037160701.csv')\ncheck","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sensors = set()\nobservations = set()\nnan_columns = list()\nmissed_groups = list()\nfor_df = list()\n\nfor item in train_frags:\n    name = int(item.split('.')[-2].split('/')[-1])\n    at_least_one_missed = 0\n    frag = pd.read_csv(item)\n    missed_group = list()\n    missed_percents = list()\n    for col in frag.columns:\n        missed_percents.append(frag[col].isnull().sum() / len(frag))\n        if pd.isnull(frag[col]).all() == True:\n            at_least_one_missed = 1\n            nan_columns.append(col)\n            missed_group.append(col)\n    if len(missed_group) > 0:\n        missed_groups.append(missed_group)\n    sensors.add(len(frag.columns))\n    observations.add(len(frag))\n    for_df.append([name, at_least_one_missed] + missed_percents)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"absent_df = pd.DataFrame(absent_groups.items(), columns=['Group', 'Missed number'])\nabsent_df = absent_df.sort_values('Missed number')\n\nfig = px.bar(\n    absent_df, \n    y=\"Group\",\n    x='Missed number',\n    orientation='h',\n    width=800,\n    height=600,\n    title='Number of missed sensor groups in training dataset'\n)\n\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for_df = pd.DataFrame(\n    for_df, \n    columns=[\n        'segment_id', 'has_missed_sensors', 'missed_percent_sensor1', \n        'missed_percent_sensor2', 'missed_percent_sensor3', 'missed_percent_sensor4', \n        'missed_percent_sensor5', 'missed_percent_sensor6', 'missed_percent_sensor7', \n        'missed_percent_sensor8', 'missed_percent_sensor9', 'missed_percent_sensor10'\n    ]\n)\n\nfor_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.merge(train, for_df)\ntrain","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_features(signal, ts, sensor_id):\n    X = pd.DataFrame()\n    f = np.fft.fft(signal)\n    f_real = np.real(f)\n    X.loc[ts, f'{sensor_id}_sum']       = signal.sum()\n    X.loc[ts, f'{sensor_id}_mean']      = signal.mean()\n    X.loc[ts, f'{sensor_id}_std']       = signal.std()\n    X.loc[ts, f'{sensor_id}_var']       = signal.var() \n    X.loc[ts, f'{sensor_id}_max']       = signal.max()\n    X.loc[ts, f'{sensor_id}_min']       = signal.min()\n    X.loc[ts, f'{sensor_id}_skew']      = signal.skew()\n    X.loc[ts, f'{sensor_id}_mad']       = signal.mad()\n    X.loc[ts, f'{sensor_id}_kurtosis']  = signal.kurtosis()\n    X.loc[ts, f'{sensor_id}_quantile99']= np.quantile(signal, 0.99)\n    X.loc[ts, f'{sensor_id}_quantile95']= np.quantile(signal, 0.95)\n    X.loc[ts, f'{sensor_id}_quantile85']= np.quantile(signal, 0.85)\n    X.loc[ts, f'{sensor_id}_quantile75']= np.quantile(signal, 0.75)\n    X.loc[ts, f'{sensor_id}_quantile55']= np.quantile(signal, 0.55)\n    X.loc[ts, f'{sensor_id}_quantile45']= np.quantile(signal, 0.45) \n    X.loc[ts, f'{sensor_id}_quantile25']= np.quantile(signal, 0.25) \n    X.loc[ts, f'{sensor_id}_quantile15']= np.quantile(signal, 0.15) \n    X.loc[ts, f'{sensor_id}_quantile05']= np.quantile(signal, 0.05)\n    X.loc[ts, f'{sensor_id}_quantile01']= np.quantile(signal, 0.01)\n    X.loc[ts, f'{sensor_id}_fft_real_mean']= f_real.mean()\n    X.loc[ts, f'{sensor_id}_fft_real_std'] = f_real.std()\n    X.loc[ts, f'{sensor_id}_fft_real_max'] = f_real.max()\n    X.loc[ts, f'{sensor_id}_fft_real_min'] = f_real.min()\n\n    return X","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = list()\nj=0\nfor seg in train.segment_id:\n    signals = pd.read_csv(f'/kaggle/input/predict-volcanic-eruptions-ingv-oe/train/{seg}.csv')\n    train_row = []\n    if j%500 == 0:\n        print(j)\n    for i in range(0, 10):\n        sensor_id = f'sensor_{i+1}'\n        train_row.append(build_features(signals[sensor_id].fillna(0), seg, sensor_id))\n    train_row = pd.concat(train_row, axis=1)\n    train_set.append(train_row)\n    j+=1\n\ntrain_set = pd.concat(train_set)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = train_set.reset_index()\ntrain_set = train_set.rename(columns={'index': 'segment_id'})\ntrain_set = pd.merge(train_set, train, on='segment_id')\ntrain_set","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_cols = list()\nfor col in train_set.columns:\n    if col == 'segment_id':\n        continue\n    if abs(train_set[col].corr(train_set['time_to_eruption'])) < 0.01:\n        drop_cols.append(col)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"not_to_drop_cols = list()\n\nfor col1 in train_set.columns:\n    for col2 in train_set.columns:\n        if col1 == col2:\n            continue\n        if col1 == 'segment_id' or col2 == 'segment_id': \n            continue\n        if col1 == 'time_to_eruption' or col2 == 'time_to_eruption':\n            continue\n        if abs(train_set[col1].corr(train_set[col2])) > 0.98:\n            if col2 not in drop_cols and col1 not in not_to_drop_cols:\n                drop_cols.append(col2)\n                not_to_drop_cols.append(col1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train_set.drop(['segment_id', 'time_to_eruption'], axis=1)\ny = train_set['time_to_eruption']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduced_y = y.copy()\nreduced_train = train.copy()\nreduced_train = reduced_train.drop(drop_cols, axis=1)\nreduced_train","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, val, y, y_val = train_test_split(train, y, random_state=666, test_size=0.2, shuffle=True)\nreduced_train, reduced_val, reduced_y, reduced_y_val = train_test_split(reduced_train, reduced_y, random_state=666, test_size=0.2, shuffle=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainloader = torch.utils.data.DataLLoader(reduced_train, batch_size=32, shuffle=True) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Autoencoder for Compression","metadata":{}},{"cell_type":"code","source":"class AE(torch.nn.module):\n    \n    def __init__(self, output_units): \n        \n        super().__init__()\n        self.encoder = torch.nn.Sequential(\n        torch.nn.ReLu(75,36),\n        torch.nn.ReLu(36,18)            \n        )\n        \n        self.decoder = torch.nn.Sequentual(\n        torch.nn.ReLu(18,36),\n        torch.nn.Sigmoid(36,output_units)      \n        )\n        \ndef call(self, inputs):\n    \n    encoded = self.encoder(inputs)\n    decoded = self.decoder(encoded)\n    return decoded\n\nauto_encoder = AE(len(reduced_train))\nloss_fn = torch.nn.L1Loss()\noptimizer = torch.optim.Adam(model.parameters(),\n                             lr = 1e-1,\n                             weight_decay = 1e-8)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(20): \n    for i, data in enumerate(trainloader, 0): \n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n\n\nencoder_layer = auto_encoder.get_layer('sequential')\nreduced_df = pd.DataFrame(encoder_layer.predict(reduced_train))\nreduced_df = reduced_df.add_prefix('feature_')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ![](http://cdn.britannica.com/34/231234-050-5B2280BB/volcanic-eruption-Antigua-Guatemala-volcano.jpg) Step 2: The Model","metadata":{}},{"cell_type":"code","source":"class Net(nn.Module):\n \n  def __init__(self):\n    super().__init__()\n    self.layers = nn.Sequential(\n      nn.Linear(,16),\n      nn.ReLU(16,8),\n      nn.Linear(),\n      nn.ReLU(),\n      nn.Linear(1)\n    )\n    self.dropout = nn.dropout(0.25)\n\n\n  def forward(self, X):\n    X = self.layers(X)\n    X = self.dropout(X)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:06:51.620315Z","iopub.execute_input":"2022-06-16T07:06:51.620725Z","iopub.status.idle":"2022-06-16T07:06:52.783025Z","shell.execute_reply.started":"2022-06-16T07:06:51.620692Z","shell.execute_reply":"2022-06-16T07:06:52.782273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mlp = MLP()\nloss_fn = nn.L1Loss()\noptimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(0, 5): \n    \n    print(f'Starting epoch {epoch+1}')\n    \n    current_loss = 0.0\n    \n    for i, data in enumerate(trainloader, 0):\n      \n      inputs, targets = data\n      inputs, targets = inputs.float(), targets.float()\n      targets = targets.reshape((targets.shape[0], 1))\n      \n      optimizer.zero_grad()\n      \n      outputs = mlp(inputs)\n      \n      loss = loss_function(outputs, targets)\n      \n      loss.backward()\n      \n      optimizer.step()\n      \n      current_loss += loss.item()\n      if i % 10 == 0:\n          print('Loss after mini-batch %5d: %.3f' %\n                (i + 1, current_loss / 500))\n          current_loss = 0.0\n\n  print('Training process has finished.')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}