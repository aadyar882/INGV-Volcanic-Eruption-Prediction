{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import auc, accuracy_score, confusion_matrix\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n\nimport xgboost as xgb\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = pd.read_csv('...input/predict-volcanic-eruptions-ingv-oe/train.csv')\ntimes = y.iloc[:,1]\ntimes.head()\n\n#final shape of training data: 4431 x 75","metadata":{"execution":{"iopub.status.busy":"2022-06-22T20:29:47.754144Z","iopub.execute_input":"2022-06-22T20:29:47.754648Z","iopub.status.idle":"2022-06-22T20:29:50.581382Z","shell.execute_reply.started":"2022-06-22T20:29:47.754611Z","shell.execute_reply":"2022-06-22T20:29:50.580177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nprint(\"Setup Complete\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* * # **Feature Engineering** adapted from INGV - Volcanic Eruption Prediction. EDA. Modeling","metadata":{}},{"cell_type":"code","source":"train_frags = glob.glob(\"../input/predict-volcanic-eruptions-ingv-oe/train/*\")\ntest_frags = glob.glob(\"../input/predict-volcanic-eruptions-ingv-oe/test/*\")\ncheck = pd.read_csv('../input/predict-volcanic-eruptions-ingv-oe/train/2037160701.csv')\ncheck","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sensors = set()\nobservations = set()\nnan_columns = list()\nmissed_groups = list()\nfor_df = list()\n\nfor item in train_frags:\n    name = int(item.split('.')[-2].split('/')[-1])\n    at_least_one_missed = 0\n    frag = pd.read_csv(item)\n    missed_group = list()\n    missed_percents = list()\n    for col in frag.columns:\n        missed_percents.append(frag[col].isnull().sum() / len(frag))\n        if pd.isnull(frag[col]).all() == True:\n            at_least_one_missed = 1\n            nan_columns.append(col)\n            missed_group.append(col)\n    if len(missed_group) > 0:\n        missed_groups.append(missed_group)\n    sensors.add(len(frag.columns))\n    observations.add(len(frag))\n    for_df.append([name, at_least_one_missed] + missed_percents)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"absent_df = pd.DataFrame(absent_groups.items(), columns=['Group', 'Missed number'])\nabsent_df = absent_df.sort_values('Missed number')\n\nplt.figure(figsize=(8, 6))\nsns.set_style(\"ticks\")\nsns.set_context(\"paper\", font_scale = 0.75)\nsns.barplot(x = absent_df['Group'], y = absent_df['Missed number'])\nplt.title(\"Number of Missed Sensor Groups in Training Dataset\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for_df = pd.DataFrame(\n    for_df, \n    columns=[\n        'segment_id', 'has_missed_sensors', 'missed_percent_sensor1', \n        'missed_percent_sensor2', 'missed_percent_sensor3', 'missed_percent_sensor4', \n        'missed_percent_sensor5', 'missed_percent_sensor6', 'missed_percent_sensor7', \n        'missed_percent_sensor8', 'missed_percent_sensor9', 'missed_percent_sensor10'\n    ]\n)\n\nfor_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.merge(train, for_df)\ntrain","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_features(signal, ts, sensor_id):\n    X = pd.DataFrame()\n    f = np.fft.fft(signal)\n    f_real = np.real(f)\n    X.loc[ts, f'{sensor_id}_sum']       = signal.sum()\n    X.loc[ts, f'{sensor_id}_mean']      = signal.mean()\n    X.loc[ts, f'{sensor_id}_std']       = signal.std()\n    X.loc[ts, f'{sensor_id}_var']       = signal.var() \n    X.loc[ts, f'{sensor_id}_max']       = signal.max()\n    X.loc[ts, f'{sensor_id}_min']       = signal.min()\n    X.loc[ts, f'{sensor_id}_skew']      = signal.skew()\n    X.loc[ts, f'{sensor_id}_mad']       = signal.mad()\n    X.loc[ts, f'{sensor_id}_kurtosis']  = signal.kurtosis()\n    X.loc[ts, f'{sensor_id}_quantile99']= np.quantile(signal, 0.99)\n    X.loc[ts, f'{sensor_id}_quantile95']= np.quantile(signal, 0.95)\n    X.loc[ts, f'{sensor_id}_quantile85']= np.quantile(signal, 0.85)\n    X.loc[ts, f'{sensor_id}_quantile75']= np.quantile(signal, 0.75)\n    X.loc[ts, f'{sensor_id}_quantile55']= np.quantile(signal, 0.55)\n    X.loc[ts, f'{sensor_id}_quantile45']= np.quantile(signal, 0.45) \n    X.loc[ts, f'{sensor_id}_quantile25']= np.quantile(signal, 0.25) \n    X.loc[ts, f'{sensor_id}_quantile15']= np.quantile(signal, 0.15) \n    X.loc[ts, f'{sensor_id}_quantile05']= np.quantile(signal, 0.05)\n    X.loc[ts, f'{sensor_id}_quantile01']= np.quantile(signal, 0.01)\n    X.loc[ts, f'{sensor_id}_fft_real_mean']= f_real.mean()\n    X.loc[ts, f'{sensor_id}_fft_real_std'] = f_real.std()\n    X.loc[ts, f'{sensor_id}_fft_real_max'] = f_real.max()\n    X.loc[ts, f'{sensor_id}_fft_real_min'] = f_real.min()\n\n    return X","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = list()\nj=0\nfor seg in train.segment_id:\n    signals = pd.read_csv(f'/kaggle/input/predict-volcanic-eruptions-ingv-oe/train/{seg}.csv')\n    train_row = []\n    if j%500 == 0:\n        print(j)\n    for i in range(0, 10):\n        sensor_id = f'sensor_{i+1}'\n        train_row.append(build_features(signals[sensor_id].fillna(0), seg, sensor_id))\n    train_row = pd.concat(train_row, axis=1)\n    train_set.append(train_row)\n    j+=1\n\ntrain_set = pd.concat(train_set)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\nsample_set = train_set.iloc[:5, :20]\nsns.set_context(\"paper\", font_scale = 0.75)\nsns.heatmap(data = sample_set, annot = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = train_set.reset_index()\ntrain_set = train_set.rename(columns={'index': 'segment_id'})\ntrain_set = pd.merge(train_set, train, on='segment_id')\ntrain_set","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_cols = list()\nfor col in train_set.columns:\n    if col == 'segment_id':\n        continue\n    if abs(train_set[col].corr(train_set['time_to_eruption'])) < 0.01:\n        drop_cols.append(col)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"not_to_drop_cols = list()\n\nfor col1 in train_set.columns:\n    for col2 in train_set.columns:\n        if col1 == col2:\n            continue\n        if col1 == 'segment_id' or col2 == 'segment_id': \n            continue\n        if col1 == 'time_to_eruption' or col2 == 'time_to_eruption':\n            continue\n        if abs(train_set[col1].corr(train_set[col2])) > 0.98:\n            if col2 not in drop_cols and col1 not in not_to_drop_cols:\n                drop_cols.append(col2)\n                not_to_drop_cols.append(col1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train_set.drop(['segment_id', 'time_to_eruption'], axis=1)\ny = train_set['time_to_eruption']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduced_y = y.copy()\nreduced_train = train.copy()\nreduced_train = reduced_train.drop(drop_cols, axis=1)\nreduced_train","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = pd.concat([reduced_train, reduced_y], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost!","metadata":{}},{"cell_type":"code","source":"def test_models_cs(): \n    models_cs = dict\n    for c in arrange(0.1, 0.2, 0.3)\n        key = \n        models_cs[key] = xgb.XGBRFRegressor(n_estimators = 500,\n                           learning_rate = 0.05, \n                           reg_lambda = 0.1,\n                           eval_metric = mse, \n                           subsample = 0.9, \n                           colsample_bynode = c)\n    return models_cs\n\ndef test_models_ne(): \n    models_ne = dict\n    for n in arrange(100, 500, 1000)\n        key = \n        models_ne[key] = xgb.XGBRFRegressor(n_estimators = n,\n                           learning_rate = 0.05, \n                           reg_lambda = 0.1,\n                           eval_metric = mse, \n                           subsample = 0.9, \n                           colsample_bynode = 0.1)\n    return models_ne\n\ndef test_models_lr(): \n    models_lr = dict\n    for l in [0.05, 0.1, 0.2, 0.5]\n        key = \n        models_lr[key] = xgb.XGBRFRegressor(n_estimators = 500,\n                           learning_rate = 0.05, \n                           reg_lambda = l,\n                           eval_metric = mse, \n                           subsample = 0.9, \n                           colsample_bynode = 0.1)\n    return models_lr\n    \ndef model_eval(model, X, y): \n    cross_val = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3)\n    scores = cross_val_score(model, X, y, scoring = 'accuracy', cv = cross_val)\n    return scores\n    \nmodels_cs = test_models_cs()\nresults_cs, names_cs = list(), list()\nfor name, model in models_cs.items(): \n    scores_cs = model_eval(model, reduced_train, reduced_y)\n    results_cs.append(scores_cs)\n    names_cs.append(name)\n\nmodels_ne = test_models_ne()\nresults_ne, names_ne = list(), list()\nfor name, model in models_ne.items(): \n    scores_ne = model_eval(model, reduced_train, reduced_y)\n    results_ne.append(scores_ne)\n    names_ne.append(name)\n    \nmodels_lr = test_models_ne()\nresults_lr, names_lr = list(), list()\nfor name, model in models_lr.items(): \n    scores_lr = model_eval(model, reduced_train, reduced_y)\n    results_lr.append(scores_lr)\n    names_lr.append(name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('>%s %.3f (%.3f)' % (name, mean(scores_cs), std(scores_cs)))\nprint('>%s %.3f (%.3f)' % (name, mean(scores_ne), std(scores_ne)))\nprint('>%s %.3f (%.3f)' % (name, mean(scores_lr), std(scores_lr)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (8,8))\ngs = fig.add_gridspec(2,2)\n\nfig.add_suplot(gs[0,0])\nsns.scatterplot(x = names_cs, y = results_cs)\nsns.regplot(x = names_cs, y = results_cs)\nplt.title(\"Scores by Colsample Value\")\n\nfig.add_suplot(gs[0,1])\nsns.scatterplot(x = names_ne, y = results_ne)\nsns.regplot(x = names_ne, y = results_ne)\nplt.title(\"Scores by Num Estimators\")\n\nfig.add_suplot(gs[1,0])\nsns.scatterplot(x = names_lr, y = results_lr)\nsns.regplot(x = names_lr, y = results_lr)\nplt.title(\"Scores by Learning Rate\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}