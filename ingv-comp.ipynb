{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math \n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import auc, accuracy_score, confusion_matrix\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\nfrom glob import glob\nimport xgboost as xgb\nfrom xgboost import XGBRFRegressor \n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nprint(\"Setup Complete\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/predict-volcanic-eruptions-ingv-oe/train.csv\")\nsample_submission = pd.read_csv(\"../input/predict-volcanic-eruptions-ingv-oe/sample_submission.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* * # **Feature Engineering** adapted from INGV - Volcanic Eruption Prediction. EDA. Modeling","metadata":{}},{"cell_type":"code","source":"train_frags = glob(\"../input/predict-volcanic-eruptions-ingv-oe/train/*\")\ntest_frags = glob(\"../input/predict-volcanic-eruptions-ingv-oe/test/*\")\ncheck = pd.read_csv('../input/predict-volcanic-eruptions-ingv-oe/train/2037160701.csv')\ncheck","metadata":{"execution":{"iopub.status.busy":"2022-08-11T19:19:19.619244Z","iopub.execute_input":"2022-08-11T19:19:19.619647Z","iopub.status.idle":"2022-08-11T19:19:19.821044Z","shell.execute_reply.started":"2022-08-11T19:19:19.619615Z","shell.execute_reply":"2022-08-11T19:19:19.820044Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"       sensor_1  sensor_2  sensor_3  sensor_4  sensor_5  sensor_6  sensor_7  \\\n0        -343.0    -110.0    -274.0     367.0     137.0    -106.0    -578.0   \n1        -298.0     -92.0    -159.0     288.0      76.0    -251.0    -713.0   \n2        -503.0     195.0    -140.0     266.0    -100.0    -162.0    -450.0   \n3        -153.0     -68.0     -78.0     301.0    -143.0     -86.0    -378.0   \n4        -320.0    -348.0     -27.0     283.0     -26.0    -255.0    -595.0   \n...         ...       ...       ...       ...       ...       ...       ...   \n59996     329.0      -1.0     573.0     155.0      76.0     108.0    -727.0   \n59997    -125.0      32.0     599.0     243.0      99.0     105.0    -788.0   \n59998     403.0      17.0     555.0     360.0      98.0     -66.0    -773.0   \n59999    -212.0     -92.0     548.0     163.0      71.0     -13.0    -207.0   \n60000    -131.0    -179.0     497.0     223.0     139.0      81.0      98.0   \n\n       sensor_8  sensor_9  sensor_10  \n0         339.0    -506.0     1389.0  \n1         352.0    -407.0     1451.0  \n2         443.0    -406.0     1295.0  \n3          66.0    -472.0     1127.0  \n4        -127.0    -243.0      950.0  \n...         ...       ...        ...  \n59996    -408.0    -232.0     1015.0  \n59997    -772.0    -239.0      951.0  \n59998    -888.0    -270.0      732.0  \n59999   -1237.0    -127.0      790.0  \n60000   -1121.0    -108.0      868.0  \n\n[60001 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sensor_1</th>\n      <th>sensor_2</th>\n      <th>sensor_3</th>\n      <th>sensor_4</th>\n      <th>sensor_5</th>\n      <th>sensor_6</th>\n      <th>sensor_7</th>\n      <th>sensor_8</th>\n      <th>sensor_9</th>\n      <th>sensor_10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-343.0</td>\n      <td>-110.0</td>\n      <td>-274.0</td>\n      <td>367.0</td>\n      <td>137.0</td>\n      <td>-106.0</td>\n      <td>-578.0</td>\n      <td>339.0</td>\n      <td>-506.0</td>\n      <td>1389.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-298.0</td>\n      <td>-92.0</td>\n      <td>-159.0</td>\n      <td>288.0</td>\n      <td>76.0</td>\n      <td>-251.0</td>\n      <td>-713.0</td>\n      <td>352.0</td>\n      <td>-407.0</td>\n      <td>1451.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-503.0</td>\n      <td>195.0</td>\n      <td>-140.0</td>\n      <td>266.0</td>\n      <td>-100.0</td>\n      <td>-162.0</td>\n      <td>-450.0</td>\n      <td>443.0</td>\n      <td>-406.0</td>\n      <td>1295.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-153.0</td>\n      <td>-68.0</td>\n      <td>-78.0</td>\n      <td>301.0</td>\n      <td>-143.0</td>\n      <td>-86.0</td>\n      <td>-378.0</td>\n      <td>66.0</td>\n      <td>-472.0</td>\n      <td>1127.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-320.0</td>\n      <td>-348.0</td>\n      <td>-27.0</td>\n      <td>283.0</td>\n      <td>-26.0</td>\n      <td>-255.0</td>\n      <td>-595.0</td>\n      <td>-127.0</td>\n      <td>-243.0</td>\n      <td>950.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>59996</th>\n      <td>329.0</td>\n      <td>-1.0</td>\n      <td>573.0</td>\n      <td>155.0</td>\n      <td>76.0</td>\n      <td>108.0</td>\n      <td>-727.0</td>\n      <td>-408.0</td>\n      <td>-232.0</td>\n      <td>1015.0</td>\n    </tr>\n    <tr>\n      <th>59997</th>\n      <td>-125.0</td>\n      <td>32.0</td>\n      <td>599.0</td>\n      <td>243.0</td>\n      <td>99.0</td>\n      <td>105.0</td>\n      <td>-788.0</td>\n      <td>-772.0</td>\n      <td>-239.0</td>\n      <td>951.0</td>\n    </tr>\n    <tr>\n      <th>59998</th>\n      <td>403.0</td>\n      <td>17.0</td>\n      <td>555.0</td>\n      <td>360.0</td>\n      <td>98.0</td>\n      <td>-66.0</td>\n      <td>-773.0</td>\n      <td>-888.0</td>\n      <td>-270.0</td>\n      <td>732.0</td>\n    </tr>\n    <tr>\n      <th>59999</th>\n      <td>-212.0</td>\n      <td>-92.0</td>\n      <td>548.0</td>\n      <td>163.0</td>\n      <td>71.0</td>\n      <td>-13.0</td>\n      <td>-207.0</td>\n      <td>-1237.0</td>\n      <td>-127.0</td>\n      <td>790.0</td>\n    </tr>\n    <tr>\n      <th>60000</th>\n      <td>-131.0</td>\n      <td>-179.0</td>\n      <td>497.0</td>\n      <td>223.0</td>\n      <td>139.0</td>\n      <td>81.0</td>\n      <td>98.0</td>\n      <td>-1121.0</td>\n      <td>-108.0</td>\n      <td>868.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>60001 rows Ã— 10 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sensors = set()\nobservations = set()\nnan_columns = list()\nmissed_groups = list()\nfor_df = list()\n\nfor item in train_frags:\n    name = int(item.split('.')[-2].split('/')[-1])\n    at_least_one_missed = 0\n    frag = pd.read_csv(item)\n    missed_group = list()\n    missed_percents = list()\n    for col in frag.columns:\n        missed_percents.append(frag[col].isnull().sum() / len(frag))\n        if pd.isnull(frag[col]).all() == True:\n            at_least_one_missed = 1\n            nan_columns.append(col)\n            missed_group.append(col)\n    if len(missed_group) > 0:\n        missed_groups.append(missed_group)\n    sensors.add(len(frag.columns))\n    observations.add(len(frag))\n    for_df.append([name, at_least_one_missed] + missed_percents)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T19:19:55.379868Z","iopub.execute_input":"2022-08-11T19:19:55.380278Z","iopub.status.idle":"2022-08-11T19:26:58.500782Z","shell.execute_reply.started":"2022-08-11T19:19:55.380245Z","shell.execute_reply":"2022-08-11T19:26:58.497284Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/471191859.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mat_least_one_missed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mfrag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmissed_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmissed_percents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."],"ename":"ParserError","evalue":"Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.","output_type":"error"}]},{"cell_type":"code","source":"absent_df = pd.DataFrame(absent_groups.items(), columns=['Group', 'Missed number'])\nabsent_df = absent_df.sort_values('Missed number')\n\nplt.figure(figsize=(8, 6))\nsns.set_style(\"ticks\")\nsns.set_context(\"paper\", font_scale = 0.75)\nsns.barplot(x = absent_df['Group'], y = absent_df['Missed number'])\nplt.title(\"Number of Missed Sensor Groups in Training Dataset\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for_df = pd.DataFrame(\n    for_df, \n    columns=[\n        'segment_id', 'has_missed_sensors', 'missed_percent_sensor1', \n        'missed_percent_sensor2', 'missed_percent_sensor3', 'missed_percent_sensor4', \n        'missed_percent_sensor5', 'missed_percent_sensor6', 'missed_percent_sensor7', \n        'missed_percent_sensor8', 'missed_percent_sensor9', 'missed_percent_sensor10'\n    ]\n)\n\nfor_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.merge(train, for_df)\ntrain","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_features(signal, ts, sensor_id):\n    X = pd.DataFrame()\n    f = np.fft.fft(signal)\n    f_real = np.real(f)\n    X.loc[ts, f'{sensor_id}_sum']       = signal.sum()\n    X.loc[ts, f'{sensor_id}_mean']      = signal.mean()\n    X.loc[ts, f'{sensor_id}_std']       = signal.std()\n    X.loc[ts, f'{sensor_id}_var']       = signal.var() \n    X.loc[ts, f'{sensor_id}_max']       = signal.max()\n    X.loc[ts, f'{sensor_id}_min']       = signal.min()\n    X.loc[ts, f'{sensor_id}_skew']      = signal.skew()\n    X.loc[ts, f'{sensor_id}_mad']       = signal.mad()\n    X.loc[ts, f'{sensor_id}_kurtosis']  = signal.kurtosis()\n    X.loc[ts, f'{sensor_id}_quantile99']= np.quantile(signal, 0.99)\n    X.loc[ts, f'{sensor_id}_quantile95']= np.quantile(signal, 0.95)\n    X.loc[ts, f'{sensor_id}_quantile85']= np.quantile(signal, 0.85)\n    X.loc[ts, f'{sensor_id}_quantile75']= np.quantile(signal, 0.75)\n    X.loc[ts, f'{sensor_id}_quantile55']= np.quantile(signal, 0.55)\n    X.loc[ts, f'{sensor_id}_quantile45']= np.quantile(signal, 0.45) \n    X.loc[ts, f'{sensor_id}_quantile25']= np.quantile(signal, 0.25) \n    X.loc[ts, f'{sensor_id}_quantile15']= np.quantile(signal, 0.15) \n    X.loc[ts, f'{sensor_id}_quantile05']= np.quantile(signal, 0.05)\n    X.loc[ts, f'{sensor_id}_quantile01']= np.quantile(signal, 0.01)\n    X.loc[ts, f'{sensor_id}_fft_real_mean']= f_real.mean()\n    X.loc[ts, f'{sensor_id}_fft_real_std'] = f_real.std()\n    X.loc[ts, f'{sensor_id}_fft_real_max'] = f_real.max()\n    X.loc[ts, f'{sensor_id}_fft_real_min'] = f_real.min()\n\n    return X","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = list()\nj=0\nfor seg in train.segment_id:\n    signals = pd.read_csv(f'/kaggle/input/predict-volcanic-eruptions-ingv-oe/train/{seg}.csv')\n    train_row = []\n    if j%500 == 0:\n        print(j)\n    for i in range(0, 10):\n        sensor_id = f'sensor_{i+1}'\n        train_row.append(build_features(signals[sensor_id].fillna(0), seg, sensor_id))\n    train_row = pd.concat(train_row, axis=1)\n    train_set.append(train_row)\n    j+=1\n\ntrain_set = pd.concat(train_set)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\nsample_set = train_set.iloc[:5, :20]\nsns.set_context(\"paper\", font_scale = 0.75)\nsns.heatmap(data = sample_set, annot = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = train_set.reset_index()\ntrain_set = train_set.rename(columns={'index': 'segment_id'})\ntrain_set = pd.merge(train_set, train, on='segment_id')\ntrain_set","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_cols = list()\nfor col in train_set.columns:\n    if col == 'segment_id':\n        continue\n    if abs(train_set[col].corr(train_set['time_to_eruption'])) < 0.01:\n        drop_cols.append(col)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"not_to_drop_cols = list()\n\nfor col1 in train_set.columns:\n    for col2 in train_set.columns:\n        if col1 == col2:\n            continue\n        if col1 == 'segment_id' or col2 == 'segment_id': \n            continue\n        if col1 == 'time_to_eruption' or col2 == 'time_to_eruption':\n            continue\n        if abs(train_set[col1].corr(train_set[col2])) > 0.98:\n            if col2 not in drop_cols and col1 not in not_to_drop_cols:\n                drop_cols.append(col2)\n                not_to_drop_cols.append(col1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train_set.drop(['segment_id', 'time_to_eruption'], axis=1)\ny = train_set['time_to_eruption']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduced_y = y.copy()\nreduced_train = train.copy()\nreduced_train = reduced_train.drop(drop_cols, axis=1)\nreduced_train","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = pd.concat([reduced_train, reduced_y], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost!","metadata":{}},{"cell_type":"code","source":"def test_models_cs(): \n    models_cs = dict\n    for c in np.arrange(0.05, 0.1, 0.01)\n        key = \n        models_cs[key] = xgb.XGBRFRegressor(n_estimators = 500,\n                           learning_rate = 0.05, \n                           reg_lambda = 0.1,\n                           eval_metric = mean_absolute_error, \n                           subsample = 0.9, \n                           colsample_bynode = c)\n    return models_cs\n\ndef test_models_ne(): \n    models_ne = dict\n    for n in arrange(100, 1000, 100)\n        key = \n        models_ne[key] = xgb.XGBRFRegressor(n_estimators = n,\n                           learning_rate = 0.05, \n                           reg_lambda = 0.1,\n                           eval_metric = mean_absolute_error, \n                           subsample = 0.9, \n                           colsample_bynode = 0.1)\n    return models_ne\n\ndef test_models_lr(): \n    models_lr = dict()\n    for l in arrange(0.05, 0.2, 0.05)\n        key = \n        models_lr[key] = xgb.XGBRFRegressor(n_estimators = 500,\n                           learning_rate = l, \n                           reg_lambda = 0.1,\n                           eval_metric = mean_absolute_error, \n                           subsample = 0.9, \n                           colsample_bynode = 0.1)\n    return models_lr\n    \ndef model_eval(model, X, y): \n    cross_val = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3)\n    scores = cross_val_score(model, X, y, scoring = 'accuracy', cv = cross_val)\n    return scores\n    \nmodels_cs = test_models_cs()\nresults_cs, names_cs = list(), list()\nfor name, model in models_cs.items(): \n    scores_cs = model_eval(model, reduced_train, reduced_y)\n    results_cs.append(scores_cs)\n    names_cs.append(name)\n\nmodels_ne = test_models_ne()\nresults_ne, names_ne = list(), list()\nfor name, model in models_ne.items(): \n    scores_ne = model_eval(model, reduced_train, reduced_y)\n    results_ne.append(scores_ne)\n    names_ne.append(name)\n    \nmodels_lr = test_models_ne()\nresults_lr, names_lr = list(), list()\nfor name, model in models_lr.items(): \n    scores_lr = model_eval(model, reduced_train, reduced_y)\n    results_lr.append(scores_lr)\n    names_lr.append(name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('>%s %.3f (%.3f)' % (name, mean(scores_cs), std(scores_cs)))\nprint('>%s %.3f (%.3f)' % (name, mean(scores_ne), std(scores_ne)))\nprint('>%s %.3f (%.3f)' % (name, mean(scores_lr), std(scores_lr)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (8,8))\ngs = fig.add_gridspec(2,2)\n\nfig.add_suplot(gs[0,0])\nsns.scatterplot(x = names_cs, y = results_cs)\nsns.regplot(x = names_cs, y = results_cs)\nplt.title(\"Scores by Colsample Value\")\n\nfig.add_suplot(gs[0,1])\nsns.scatterplot(x = names_ne, y = results_ne)\nsns.regplot(x = names_ne, y = results_ne)\nplt.title(\"Scores by Num Estimators\")\n\nfig.add_suplot(gs[1,0])\nsns.scatterplot(x = names_lr, y = results_lr)\nsns.regplot(x = names_lr, y = results_lr)\nplt.title(\"Scores by Learning Rate\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}